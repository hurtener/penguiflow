"""Planner configuration for {{ project_name }}."""

from __future__ import annotations

import json
from collections.abc import Mapping, Sequence
from dataclasses import dataclass
from typing import Any

from penguiflow.planner import PlannerEventCallback, ReactPlanner
from penguiflow.rich_output import DEFAULT_ALLOWLIST, RichOutputConfig, attach_rich_output_nodes, get_runtime

from .config import Config
from .tools import build_catalog_bundle


SYSTEM_PROMPT_EXTRA = ""


def _build_system_prompt(rich_output_prompt: str) -> str:
    prompt = SYSTEM_PROMPT_EXTRA
    if rich_output_prompt:
        prompt = f"{prompt}\n\n{rich_output_prompt}" if prompt else rich_output_prompt
    return prompt


@dataclass
class PlannerBundle:
    """Container for planner and LLM client."""

    planner: ReactPlanner
    llm_client: "ScriptedLLM"


class ScriptedLLM:
    """Deterministic LLM client that returns wayfinder planner actions."""

    def __init__(self, scripted: Sequence[Mapping[str, Any]] | None = None) -> None:
        self._scripted = [json.dumps(item, ensure_ascii=False) for item in scripted] if scripted else None

    async def complete(
        self,
        *,
        messages: list[Mapping[str, str]],
        response_format: Mapping[str, Any] | None = None,
        stream: bool = False,
        on_stream_chunk: object = None,
    ) -> str:
        del response_format, stream, on_stream_chunk
        if self._scripted is None:
            query = messages[-1].get("content", "")
            scripted = [
                {
                    "thought": "plan query",
                    "next_node": "plan_nlq",
                    "args": {"question": query},
                },
                {
                    "thought": "execute sql",
                    "next_node": "execute_sql",
                    "args": {"sql": f"SELECT * FROM stub WHERE question = '{query}'"},
                },
                {
                    "thought": "finish",
                    "next_node": None,
                    "args": None,
                },
            ]
            self._scripted = [json.dumps(item, ensure_ascii=False) for item in scripted]

        if not self._scripted:
            raise RuntimeError("ScriptedLLM has no responses left")

        return self._scripted.pop(0)


def _build_rich_output_config(config: Config) -> RichOutputConfig:
    allowlist = config.rich_output_allowlist or list(DEFAULT_ALLOWLIST)
    return RichOutputConfig(
        enabled=config.rich_output_enabled,
        allowlist=allowlist,
        include_prompt_catalog=config.rich_output_include_prompt_catalog,
        include_prompt_examples=config.rich_output_include_prompt_examples,
        max_payload_bytes=config.rich_output_max_payload_bytes,
        max_total_bytes=config.rich_output_max_total_bytes,
    )


def build_planner(
    config: Config,
    *,
    event_callback: PlannerEventCallback | None = None,
) -> PlannerBundle:
    """Create a ReactPlanner wired with scripted responses.

    Swap the scripted client with a real model by:
    - LiteLLM path: pass llm=config.llm_model (e.g., "gpt-4o") to ReactPlanner and set provider keys in env
      (e.g., OPENAI_API_KEY).
    - DSPy path: pass llm_client=DSPyLLMClient(llm=config.llm_model) and leave llm=None.
    - Custom client: pass any object with .complete(messages=[...], response_format=...).
    Prompt tweaks: pass system_prompt_extra (string) or planning_hints (mapping) to ReactPlanner to append
    guidance without editing penguiflow.planner.prompts.
    """
    nodes, registry = build_catalog_bundle()
    rich_output_config = _build_rich_output_config(config)
    nodes.extend(attach_rich_output_nodes(registry, config=rich_output_config))
    rich_output_prompt = get_runtime().prompt_section()
    llm_client = ScriptedLLM()
    planner = ReactPlanner(
        llm_client=llm_client,
        nodes=nodes,
        registry=registry,
        system_prompt_extra=_build_system_prompt(rich_output_prompt),
        event_callback=event_callback,
    )
    return PlannerBundle(planner=planner, llm_client=llm_client)

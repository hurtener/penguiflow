# ============================================================================
# Enterprise Agent V2 Configuration Template - Enhanced Edition
# ============================================================================
# Copy this file to .env and configure with your credentials
#
# V2 NEW FEATURES:
# - Reflection loop for answer quality assurance (FLAGSHIP)
# - Tool policy for runtime access control
# - Planning hints for workflow constraints
# - State store for durable pause/resume

# ============================================================================
# LLM Provider Configuration (REQUIRED)
# ============================================================================

# OpenAI Configuration (recommended for production)
OPENAI_API_KEY=sk-...

# Alternative: Anthropic Claude
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-3-5-sonnet-20241022

# Alternative: Azure OpenAI
# AZURE_API_KEY=...
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-01
# LLM_MODEL=azure/gpt-4o-mini

# ============================================================================
# Planner Configuration
# ============================================================================

# Primary LLM model for planning
LLM_MODEL=gpt-4o-mini

# LLM Temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.3

# Maximum retry attempts for transient LLM failures
LLM_MAX_RETRIES=3

# Timeout per LLM API call (seconds)
LLM_TIMEOUT_S=60.0

# Maximum tokens per LLM response (controls output length and cost)
LLM_MAX_TOKENS=4096

# ==============================================================================
# LLM Client Configuration
# ==============================================================================

# Use DSPy for structured outputs (recommended for non-OpenAI models)
# DSPy provides better reliability for models without native JSON schema support
# Automatically enabled for Databricks models, or set to true for Cerebras, Groq, etc.
# Default: false (use LiteLLM with native JSON schema mode)
DSPY_CLIENT=false

# ==============================================================================
# Planner Configuration
# ==============================================================================

# Maximum planning iterations before giving up
PLANNER_MAX_ITERS=12

# Token budget for trajectory compression (0 = disabled)
PLANNER_TOKEN_BUDGET=8000

# Wall-clock deadline for planning session (seconds, optional)
# PLANNER_DEADLINE_S=30.0

# Maximum tool invocations allowed (optional)
# PLANNER_HOP_BUDGET=20

# Absolute safety limit for parallel execution
PLANNER_ABSOLUTE_MAX_PARALLEL=50

# Number of attempts to repair invalid LLM JSON responses
PLANNER_REPAIR_ATTEMPTS=3

# Cheaper model for trajectory summarization (optional)
SUMMARIZER_MODEL=gpt-4o-mini

# ============================================================================
# V2: Reflection Configuration (FLAGSHIP FEATURE)
# ============================================================================

# Enable reflection loop for answer quality assurance
REFLECTION_ENABLED=true

# Model for answer critique (uses main LLM if not set)
REFLECTION_LLM=gpt-4o-mini

# Minimum quality score to accept answer (0.0-1.0)
REFLECTION_QUALITY_THRESHOLD=0.80

# Maximum revision attempts before accepting
REFLECTION_MAX_REVISIONS=2

# Use separate LLM for reflection vs main planning
REFLECTION_USE_SEPARATE_LLM=false

# ============================================================================
# V2: Tool Policy Configuration (Runtime Access Control)
# ============================================================================

# Enable tool filtering based on policy
TOOL_POLICY_ENABLED=false

# Whitelist specific tools (comma-separated, empty = all allowed)
# Example: TOOL_POLICY_ALLOWED_TOOLS=triage_query,analyze_documents,answer_general
# TOOL_POLICY_ALLOWED_TOOLS=

# Blacklist specific tools (comma-separated)
# Example: TOOL_POLICY_DENIED_TOOLS=expensive_tool,deprecated_node
# TOOL_POLICY_DENIED_TOOLS=

# Require all listed tags for tool availability (comma-separated)
# Example: TOOL_POLICY_REQUIRE_TAGS=safe,read-only
# TOOL_POLICY_REQUIRE_TAGS=

# ============================================================================
# V2: Planning Hints Configuration (Workflow Constraints)
# ============================================================================

# Enable planning hints for guiding planner behavior
PLANNING_HINTS_ENABLED=false

# JSON object with hints (ordering, parallel groups, constraints)
# Example: PLANNING_HINTS='{"ordering_hints":["triage","retrieve"],"max_parallel":3}'
# PLANNING_HINTS=

# ============================================================================
# V2: State Store Configuration (Durable Pause/Resume)
# ============================================================================

# Enable state persistence for pause/resume across restarts
STATE_STORE_ENABLED=false

# Backend for state persistence: memory, redis, sqlite
STATE_STORE_BACKEND=memory

# ============================================================================
# Observability Configuration
# ============================================================================

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable telemetry collection and emission
ENABLE_TELEMETRY=true

# Telemetry backend: logging, mlflow, datadog
TELEMETRY_BACKEND=logging

# MLflow tracking server (if using mlflow backend)
# MLFLOW_TRACKING_URI=http://localhost:5000

# ============================================================================
# Application Settings
# ============================================================================

# Environment: development, staging, production
AGENT_ENVIRONMENT=development

# Agent identifier for logging and metrics
AGENT_NAME=enterprise_agent_v2

# ============================================================================
# Production Deployment Notes
# ============================================================================
#
# For production deployments:
#
# 1. Security:
#    - Store credentials in secrets manager (AWS Secrets Manager, HashiCorp Vault)
#    - Never commit .env files to version control
#    - Rotate API keys regularly
#    - Use least-privilege service accounts
#
# 2. Cost Management:
#    - Set PLANNER_TOKEN_BUDGET to limit LLM costs
#    - Use SUMMARIZER_MODEL with cheaper model (gpt-4o-mini vs gpt-4)
#    - Monitor PLANNER_HOP_BUDGET to prevent runaway executions
#
# 3. Performance:
#    - Adjust PLANNER_MAX_ITERS based on task complexity
#    - Set PLANNER_DEADLINE_S for user-facing interactions
#    - Tune LLM_TIMEOUT_S based on provider SLAs
#
# 4. Reliability:
#    - Set LLM_MAX_RETRIES=3 for transient failures
#    - Use circuit breaker patterns for degraded LLM service
#    - Configure health checks for readiness probes
#
# 5. Observability:
#    - Set LOG_LEVEL=INFO in production
#    - Enable TELEMETRY_BACKEND=mlflow or datadog
#    - Configure structured logging for aggregation
#    - Set up alerting for error rates and latency
#
# 6. V2 Features (New):
#    - Enable REFLECTION_ENABLED=true for quality assurance
#    - Use TOOL_POLICY for multi-tenant tool isolation
#    - Set PLANNING_HINTS for deterministic workflows
#    - Enable STATE_STORE with Redis/SQLite for durable pause/resume
#    - Monitor reflection events for answer quality metrics
#
# ============================================================================
#
# V2 CONFIGURATION PRESETS
# ============================================================================
#
# High-Quality Preset (best answers, higher cost):
# - REFLECTION_ENABLED=true
# - REFLECTION_QUALITY_THRESHOLD=0.85
# - LLM_MODEL=gpt-4o
# - REFLECTION_LLM=gpt-4o
#
# Cost-Optimized Preset (faster, lower cost):
# - REFLECTION_ENABLED=false
# - LLM_MODEL=gpt-4o-mini
# - SUMMARIZER_MODEL=gpt-4o-mini
#
# Multi-Tenant Preset (tool isolation):
# - TOOL_POLICY_ENABLED=true
# - TOOL_POLICY_ALLOWED_TOOLS=triage_query,analyze_documents
#
# ============================================================================

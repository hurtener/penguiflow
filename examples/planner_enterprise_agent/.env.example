# ============================================================================
# Enterprise Agent Configuration Template
# ============================================================================
# Copy this file to .env and configure with your credentials

# ============================================================================
# LLM Provider Configuration (REQUIRED)
# ============================================================================

# OpenAI Configuration (recommended for production)
OPENAI_API_KEY=sk-...

# Alternative: Anthropic Claude
# ANTHROPIC_API_KEY=sk-ant-...
# LLM_MODEL=claude-3-5-sonnet-20241022

# Alternative: Azure OpenAI
# AZURE_API_KEY=...
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-01
# LLM_MODEL=azure/gpt-4o-mini

# ============================================================================
# Planner Configuration
# ============================================================================

# Primary LLM model for planning
LLM_MODEL=gpt-4o-mini

# LLM Temperature (0.0 = deterministic, 1.0 = creative)
LLM_TEMPERATURE=0.0

# Maximum retry attempts for transient LLM failures
LLM_MAX_RETRIES=3

# Timeout per LLM API call (seconds)
LLM_TIMEOUT_S=60.0

# Maximum planning iterations before giving up
PLANNER_MAX_ITERS=12

# Token budget for trajectory compression (0 = disabled)
PLANNER_TOKEN_BUDGET=8000

# Wall-clock deadline for planning session (seconds, optional)
# PLANNER_DEADLINE_S=30.0

# Maximum tool invocations allowed (optional)
# PLANNER_HOP_BUDGET=20

# Absolute safety limit for parallel execution
PLANNER_ABSOLUTE_MAX_PARALLEL=50

# Cheaper model for trajectory summarization (optional)
SUMMARIZER_MODEL=gpt-4o-mini

# ============================================================================
# Observability Configuration
# ============================================================================

# Logging level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable telemetry collection and emission
ENABLE_TELEMETRY=true

# Telemetry backend: logging, mlflow, datadog
TELEMETRY_BACKEND=logging

# MLflow tracking server (if using mlflow backend)
# MLFLOW_TRACKING_URI=http://localhost:5000

# ============================================================================
# Application Settings
# ============================================================================

# Environment: development, staging, production
AGENT_ENVIRONMENT=development

# Agent identifier for logging and metrics
AGENT_NAME=enterprise_agent

# ============================================================================
# Production Deployment Notes
# ============================================================================
#
# For production deployments:
#
# 1. Security:
#    - Store credentials in secrets manager (AWS Secrets Manager, HashiCorp Vault)
#    - Never commit .env files to version control
#    - Rotate API keys regularly
#    - Use least-privilege service accounts
#
# 2. Cost Management:
#    - Set PLANNER_TOKEN_BUDGET to limit LLM costs
#    - Use SUMMARIZER_MODEL with cheaper model (gpt-4o-mini vs gpt-4)
#    - Monitor PLANNER_HOP_BUDGET to prevent runaway executions
#
# 3. Performance:
#    - Adjust PLANNER_MAX_ITERS based on task complexity
#    - Set PLANNER_DEADLINE_S for user-facing interactions
#    - Tune LLM_TIMEOUT_S based on provider SLAs
#
# 4. Reliability:
#    - Set LLM_MAX_RETRIES=3 for transient failures
#    - Use circuit breaker patterns for degraded LLM service
#    - Configure health checks for readiness probes
#
# 5. Observability:
#    - Set LOG_LEVEL=INFO in production
#    - Enable TELEMETRY_BACKEND=mlflow or datadog
#    - Configure structured logging for aggregation
#    - Set up alerting for error rates and latency
#
# ============================================================================
